{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpfG3+P6SJu9p8QRP0Oa23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fansie1/chemRxnResource/blob/main/chemrxnresource.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hACkvkomBz4i"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval\n",
        "!pip install transformers\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertForTokenClassification\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "class BertForTagging(BertForTokenClassification):\n",
        "    def __init__(self, config, use_cls=False):\n",
        "        super(BertForTagging, self).__init__(config)\n",
        "\n",
        "        self.use_cls = use_cls\n",
        "        if self.use_cls:\n",
        "            self.classifier = nn.Linear(config.hidden_size * 2, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "#         pdb.set_trace()\n",
        "        if self.use_cls:\n",
        "            batch_size, seq_length, hidden_dim = sequence_output.shape\n",
        "            extended_cls_h = outputs[1].unsqueeze(1).expand(batch_size, seq_length, hidden_dim)\n",
        "            sequence_output = torch.cat([sequence_output, extended_cls_h], 2)\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "    def decode(self, logits, mask):\n",
        "        preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "        batch_size, seq_len = preds.shape\n",
        "        preds_list = [[] for _ in range(batch_size)]\n",
        "        for i in range(batch_size):\n",
        "            for j in range(seq_len):\n",
        "                if mask[i, j]:\n",
        "                    preds_list[i].append(preds[i,j])\n",
        "        return preds_list"
      ],
      "metadata": {
        "id": "G2JvApbnCGDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# шонч╗Г\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from typing import Any, Callable, Optional\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from transformers import Trainer\n",
        "from transformers import PreTrainedModel\n",
        "# from transformers import is_wandb_available\n",
        "from transformers import TrainingArguments\n",
        "from transformers.data.data_collator import DataCollator\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class IETrainer(Trainer):\n",
        "    \"\"\"\n",
        "    IETrainer is inheritated from from transformers.Trainer, optimized for IE tasks.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        args: TrainingArguments,\n",
        "        data_collator: Optional[DataCollator] = None,\n",
        "        train_dataset: Optional[Dataset] = None,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        compute_metrics=None,\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,\n",
        "        use_crf: Optional[bool]=False,\n",
        "        epoch: int = 1\n",
        "    ):\n",
        "        super(IETrainer, self).__init__(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            optimizers=optimizers\n",
        "        )\n",
        "        self.use_crf = use_crf\n",
        "        self.epoch = epoch\n",
        "        self.global_step = None\n",
        "\n",
        "    def evaluate(self, eval_dataset: Optional[Dataset] = None) -> Dict:\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self._prediction_loop(eval_dataloader, description=\"Evaluation\")\n",
        "\n",
        "        self._log(output['metrics'])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def predict(self, test_dataset: Dataset) -> Dict:\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "\n",
        "        return self._prediction_loop(test_dataloader, description=\"Prediction\")\n",
        "\n",
        "    def _prediction_loop(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        description: str\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Prediction/evaluation loop, shared by `evaluate()` and `predict()`\n",
        "        Works both with or without labels.\n",
        "        \"\"\"\n",
        "        model = self.model\n",
        "        batch_size = dataloader.batch_size\n",
        "\n",
        "        logger.info(\"***** Running %s *****\", description)\n",
        "        logger.info(\"  Num examples = %d\", self.num_examples(dataloader))\n",
        "        logger.info(\"  Batch size = %d\", batch_size)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        eval_losses: List[float] = []\n",
        "        preds_ids = []\n",
        "        label_ids = []\n",
        "\n",
        "        for inputs in tqdm(dataloader, desc=description):\n",
        "            has_labels = any(\n",
        "                inputs.get(k) is not None\n",
        "                for k in [\"labels\", \"lm_labels\", \"masked_lm_labels\"]\n",
        "            )\n",
        "\n",
        "            for k, v in inputs.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    inputs[k] = v.to(self.args.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                if has_labels:\n",
        "                    step_eval_loss, logits = outputs[:2]\n",
        "                    eval_losses += [step_eval_loss.mean().item()]\n",
        "                else:\n",
        "                    logits = outputs[0]\n",
        "\n",
        "            mask = inputs[\"decoder_mask\"].to(torch.bool)\n",
        "            preds = model.decode(logits, mask=mask)\n",
        "            preds_ids.extend(preds)\n",
        "            if inputs.get(\"labels\") is not None:\n",
        "                labels = [inputs[\"labels\"][i, mask[i]].tolist() \\\n",
        "                            for i in range(inputs[\"labels\"].shape[0])]\n",
        "                label_ids.extend(labels)\n",
        "                assert len(preds) == len(labels)\n",
        "                assert len(preds[0]) == len(labels[0])\n",
        "\n",
        "        if self.compute_metrics is not None and \\\n",
        "                len(preds_ids) > 0 and \\\n",
        "                len(label_ids) > 0:\n",
        "            metrics = self.compute_metrics(preds_ids, label_ids)\n",
        "        else:\n",
        "            metrics = {}\n",
        "        if len(eval_losses) > 0:\n",
        "            metrics['eval_loss'] = np.mean(eval_losses)\n",
        "\n",
        "        # Prefix all keys with eval_\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(\"eval_\"):\n",
        "                metrics[f\"eval_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return {'predictions': preds_ids, 'label_ids': label_ids, 'metrics': metrics}\n",
        "\n",
        "    def _log(self, logs: Dict[str, float], iterator: Optional[tqdm] = None) -> None:\n",
        "        if self.epoch is not None:\n",
        "            logs[\"epoch\"] = self.epoch\n",
        "        if self.global_step is None:\n",
        "            # when logging evaluation metrics without training\n",
        "            self.global_step = 0\n",
        "        # if is_wandb_available():\n",
        "        #     if self.is_world_master():\n",
        "        #         wandb.log(logs, step=self.global_step)\n",
        "        output = {**logs, **{\"step\": self.global_step}}\n",
        "        if iterator is not None:\n",
        "            iterator.write(output)\n",
        "        else:\n",
        "            logger.info(\n",
        "                {k:round(v, 4) if isinstance(v, float) else v for k, v in output.items()}\n",
        "            )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXJNaHwkCtOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from typing import List, Optional, Union\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class InputExample:\n",
        "    \"\"\"\n",
        "    A single training/test example for token classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        words: list. The words of the sequence.\n",
        "        labels: (Optional) list. The labels for each word of the sequence. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    guid: str\n",
        "    words: List[str]\n",
        "    metainfo: Optional[str] = None\n",
        "    labels: Optional[List[str]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class InputFeatures:\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    Property names are the same names as the corresponding inputs to a model.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids: List[int]\n",
        "    attention_mask: List[int]\n",
        "    token_type_ids: Optional[List[int]] = None\n",
        "    label_ids: Optional[List[int]] = None\n",
        "    decoder_mask: Optional[List[bool]] = None\n",
        "\n",
        "class ProdDataset(Dataset):\n",
        "    features: List[InputFeatures]\n",
        "    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n",
        "    # Use cross entropy ignore_index as padding label id so that only\n",
        "    # real label ids contribute to the loss later.\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_file: str,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        labels: List[str],\n",
        "        model_type: str,\n",
        "        max_seq_length: Optional[int] = None,\n",
        "        overwrite_cache=False\n",
        "    ):\n",
        "        # Load data features from cache or dataset file\n",
        "        data_dir = os.path.dirname(data_file)\n",
        "        fname = os.path.basename(data_file)\n",
        "        cached_features_file = os.path.join(\n",
        "            \"/kaggle/working/\",\n",
        "            \"cached_{}_{}_{}\".format(\n",
        "                fname,\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(max_seq_length)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "            logger.info(f\"Loading features from cached file {cached_features_file}\")\n",
        "            self.features = torch.load(cached_features_file)\n",
        "        else:\n",
        "            logger.info(f\"Creating features from dataset file at {data_file}\")\n",
        "            examples = read_examples_from_file(data_file)\n",
        "            self.features = convert_examples_to_features(\n",
        "                examples,\n",
        "                labels,\n",
        "                max_seq_length,\n",
        "                tokenizer,\n",
        "                cls_token=tokenizer.cls_token,\n",
        "                cls_token_segment_id=0,\n",
        "                sep_token=tokenizer.sep_token,\n",
        "                pad_token=tokenizer.pad_token_id,\n",
        "                pad_token_segment_id=tokenizer.pad_token_type_id,\n",
        "                pad_token_label_id=self.pad_token_label_id,\n",
        "            )\n",
        "            logger.info(f\"Saving features into cached file {cached_features_file}\")\n",
        "            torch.save(self.features, cached_features_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, i) -> InputFeatures:\n",
        "        return self.features[i]\n",
        "\n",
        "def read_examples_from_file(file_path) -> List[InputExample]:\n",
        "    guid_index = 1\n",
        "    examples = []\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        words, labels = [], []\n",
        "        metainfo = None\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line.startswith(\"#\\tpassage\"):\n",
        "                metainfo = line\n",
        "            elif line == \"\":\n",
        "                if words:\n",
        "                    examples.append(InputExample(\n",
        "                        guid=f\"{guid_index}\",\n",
        "                        words=words,\n",
        "                        metainfo=metainfo,\n",
        "                        labels=labels\n",
        "                    ))\n",
        "                    guid_index += 1\n",
        "                    words, labels = [], []\n",
        "            else:\n",
        "                splits = line.split(\"\\t\")\n",
        "                words.append(splits[0])\n",
        "                if len(splits) > 1:\n",
        "                    labels.append(splits[-1])\n",
        "                else:\n",
        "                    # Examples could have no label for plain test files\n",
        "                    labels.append(\"O\")\n",
        "        if words:\n",
        "            examples.append(InputExample(\n",
        "                guid=f\"{guid_index}\",\n",
        "                words=words,\n",
        "                metainfo=metainfo,\n",
        "                labels=labels\n",
        "            ))\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples: List[InputExample],\n",
        "    label_list: List[str],\n",
        "    max_seq_length: int,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    cls_token=\"[CLS]\",\n",
        "    cls_token_segment_id=0,\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    pad_token_label_id=-100,\n",
        "    sequence_a_segment_id=0,\n",
        "    sequence_b_segment_id=1,\n",
        "    mask_padding_with_zero=True,\n",
        "    verbose=False\n",
        ") -> List[InputFeatures]:\n",
        "    \"\"\" Loads a data file into a list of `InputFeatures`\n",
        "    \"\"\"\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10_000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
        "\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "        for word, label in zip(example.words, example.labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            # word_tokens = word_tokens[:5]\n",
        "\n",
        "            if len(word_tokens) > 0:\n",
        "                tokens.extend(word_tokens)\n",
        "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        if len(tokens) > max_seq_length - 2:\n",
        "            logger.warning(\"Sequence length exceed {} (cut).\".format(max_seq_length))\n",
        "            tokens = tokens[: (max_seq_length - 2)]\n",
        "            label_ids = label_ids[: (max_seq_length - 2)]\n",
        "\n",
        "        tokens += [sep_token]\n",
        "        label_ids += [pad_token_label_id]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_ids = [pad_token_label_id] + label_ids\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        seq_length = len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        input_ids += [pad_token] * padding_length\n",
        "        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
        "        segment_ids += [pad_token_segment_id] * padding_length\n",
        "        label_ids += [pad_token_label_id] * padding_length\n",
        "\n",
        "        decoder_mask = [(x != pad_token_label_id) for x in label_ids]\n",
        "\n",
        "        # assert len(input_ids) == max_seq_length\n",
        "        # assert len(input_mask) == max_seq_length\n",
        "        # assert len(segment_ids) == max_seq_length\n",
        "        # assert len(label_ids) == max_seq_length\n",
        "\n",
        "        if verbose and ex_index < 1:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: {} (length: {})\".format(example.guid, seq_length))\n",
        "            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens[:seq_length]]))\n",
        "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids[:seq_length]]))\n",
        "            # logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
        "            # logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids[:seq_length]]))\n",
        "            logger.info(\"decode_mask: %s\", \" \".join([str(x) for x in decoder_mask[:seq_length]]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=input_mask,\n",
        "                token_type_ids=segment_ids,\n",
        "                label_ids=label_ids,\n",
        "                decoder_mask=decoder_mask\n",
        "            )\n",
        "        )\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "vivrNVsuCuxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "from transformers import set_seed\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def train(model_args, data_args, train_args):\n",
        "    if (\n",
        "        os.path.exists(train_args.output_dir)\n",
        "        and os.listdir(train_args.output_dir)\n",
        "        and train_args.do_train\n",
        "        and not train_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({train_args.output_dir}) already exists and is not empty.\"\n",
        "             \" Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # logger = create_logger(name=\"train_prod\", save_dir=train_args.output_dir)\n",
        "    logger.info(\"Training/evaluation parameters %s\", train_args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(train_args.seed)\n",
        "\n",
        "    # Prepare prod-ext task\n",
        "    labels = get_labels(data_args.labels)\n",
        "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        id2label=label_map,\n",
        "        label2id={label: i for i, label in enumerate(labels)},\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast,\n",
        "    )\n",
        "    model_args.use_crf = False\n",
        "    if model_args.use_crf:\n",
        "        # model = BertCRFForTagging.from_pretrained(\n",
        "        #     model_args.model_name_or_path,\n",
        "        #     config=config,\n",
        "        #     cache_dir=model_args.cache_dir,\n",
        "        #     tagging_schema=\"BIO\",\n",
        "        #     use_cls=model_args.use_cls\n",
        "        # )\n",
        "        model = BertBiLSTMCRFForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            tagging_schema=\"BIO\",\n",
        "            use_cls=model_args.use_cls,\n",
        "            hidden_dim=config.hidden_size\n",
        "        )\n",
        "    else:\n",
        "        model = BertForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_cls=model_args.use_cls\n",
        "        )\n",
        "\n",
        "    # Get datasets\n",
        "    train_dataset = (\n",
        "        ProdDataset(\n",
        "            # data_file=os.path.join(data_args.data_dir, \"train.txt\"),\n",
        "            data_file=data_args.data_dir_train,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache\n",
        "        )\n",
        "        if train_args.do_train\n",
        "        else None\n",
        "    )\n",
        "    eval_dataset = (\n",
        "        ProdDataset(\n",
        "            # data_file=os.path.join(data_args.data_dir, \"dev.txt\"),\n",
        "            data_file=data_args.data_dir_dev,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache\n",
        "        )\n",
        "        if train_args.do_eval\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    def compute_metrics(predictions, label_ids) -> Dict:\n",
        "        label_list = [[label_map[x] for x in seq] for seq in label_ids]\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "\n",
        "        return {\n",
        "            \"precision\": precision_score(label_list, preds_list),\n",
        "            \"recall\": recall_score(label_list, preds_list),\n",
        "            \"f1\": f1_score(label_list, preds_list),\n",
        "        }\n",
        "\n",
        "    metrics_fn = compute_metrics\n",
        "    dataset_len = len(train_dataset)\n",
        "    batch_size = 16\n",
        "    total_steps = (dataset_len // batch_size) * train_args.num_train_epochs if dataset_len % batch_size == 0 else \\\n",
        "        (dataset_len // batch_size + 1) * train_args.num_train_epochs\n",
        "    train_args.warmup_steps = 0.1 * total_steps\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = IETrainer(\n",
        "        model=model,\n",
        "        args=train_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=metrics_fn,\n",
        "        use_crf=model_args.use_crf,\n",
        "        optimizers=get_optimizer_grouped_parameters(\n",
        "            use_crf=model_args.use_crf,\n",
        "            args=train_args,\n",
        "            model=model,\n",
        "            num_training_steps=total_steps),\n",
        "            # num_training_steps=len(train_dataset) * train_args.num_train_epochs),\n",
        "        epoch=train_args.num_train_epochs\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if train_args.do_train:\n",
        "        trainer.train()\n",
        "        # Pass model_path to train() if continue training from an existing ckpt.\n",
        "        # trainer.train(\n",
        "        #     model_path=model_args.model_name_or_path\n",
        "        #     if os.path.isdir(model_args.model_name_or_path)\n",
        "        #     else None\n",
        "        # )\n",
        "        trainer.save_model()\n",
        "        tokenizer.save_pretrained(train_args.output_dir)\n",
        "\n",
        "    # Evaluation\n",
        "    if train_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        output = trainer.evaluate()\n",
        "        predictions = output['predictions']\n",
        "        label_ids = output['label_ids']\n",
        "        metrics = output['metrics']\n",
        "\n",
        "        output_eval_file = os.path.join(train_args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            for key, value in metrics.items():\n",
        "                logger.info(\"  %s = %s\", key, value)\n",
        "                writer.write(\"%s = %s\\n\" % (key, value))\n",
        "\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "\n",
        "        # Save predictions\n",
        "        write_predictions(\n",
        "            data_args.data_dir_dev,\n",
        "            os.path.join(train_args.output_dir, \"eval_predictions.txt\"),\n",
        "            preds_list\n",
        "        )\n",
        "\n",
        "    # Predict\n",
        "    if train_args.do_predict:\n",
        "        test_dataset = ProdDataset(\n",
        "            data_file=data_args.data_dir_test,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache,\n",
        "        )\n",
        "\n",
        "        output = trainer.predict(test_dataset)\n",
        "\n",
        "        predictions = output['predictions']\n",
        "        label_ids = output['label_ids']\n",
        "        metrics = output['metrics']\n",
        "\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "\n",
        "        output_test_results_file = os.path.join(train_args.output_dir, \"test_results.txt\")\n",
        "        with open(output_test_results_file, \"w\") as writer:\n",
        "            for key, value in metrics.items():\n",
        "                logger.info(\"  %s = %s\", key, value)\n",
        "                writer.write(\"%s = %s\\n\" % (key, value))\n",
        "\n",
        "        # Save predictions\n",
        "        write_predictions(\n",
        "            data_args.data_dir_test,\n",
        "            os.path.join(train_args.output_dir, \"test_predictions.txt\"),\n",
        "            preds_list\n",
        "        )\n",
        "\n",
        "def get_optimizer_grouped_parameters(use_crf, args, model, num_training_steps):\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    if use_crf:\n",
        "        crf = \"crf\"\n",
        "        crf_lr = args.crf_learning_rate\n",
        "        logger.info(f\"Learning rate for CRF: {crf_lr}\")\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if (not any(nd in n for nd in no_decay)) and (crf not in n)\n",
        "                ],\n",
        "                \"weight_decay\": args.weight_decay\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for p in model.crf.parameters()],\n",
        "                \"weight_decay\": args.weight_decay,\n",
        "                \"lr\": crf_lr\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay) and (not crf not in n)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "    else:\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": args.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=args.learning_rate,\n",
        "        eps=args.adam_epsilon\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=args.warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def predict(model_args, predict_args):\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # logger = create_logger(name=\"predict_prod\", save_dir=train_args.output_dir)\n",
        "    logger.info(\"Predict parameters %s\", predict_args)\n",
        "\n",
        "    # Prepare prod-ext task\n",
        "    labels = get_labels(predict_args.labels)\n",
        "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        id2label=label_map,\n",
        "        label2id={label: i for i, label in enumerate(labels)},\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast,\n",
        "    )\n",
        "    if model_args.use_crf:\n",
        "        # model = BertCRFForTagging.from_pretrained(\n",
        "        #     model_args.model_name_or_path,\n",
        "        #     config=config,\n",
        "        #     cache_dir=model_args.cache_dir,\n",
        "        #     tagging_schema=\"BIO\"\n",
        "        # )\n",
        "        model = BertBiLSTMCRFForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            tagging_schema=\"BIO\",\n",
        "            use_cls=model_args.use_cls,\n",
        "            hidden_dim=config.hidden_size\n",
        "        )\n",
        "    else:\n",
        "        model = BertForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir\n",
        "        )\n",
        "\n",
        "    device = torch.device(\n",
        "                \"cuda\"\n",
        "                if (not predict_args.no_cuda and torch.cuda.is_available())\n",
        "                else \"cpu\"\n",
        "            )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # load test dataset\n",
        "    test_dataset = ProdDataset(\n",
        "        data_file=predict_args.input_file,\n",
        "        tokenizer=tokenizer,\n",
        "        labels=labels,\n",
        "        model_type=config.model_type,\n",
        "        max_seq_length=predict_args.max_seq_length,\n",
        "        overwrite_cache=predict_args.overwrite_cache,\n",
        "    )\n",
        "\n",
        "    sampler = SequentialSampler(test_dataset)\n",
        "    data_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=sampler,\n",
        "        batch_size=predict_args.batch_size,\n",
        "        collate_fn=default_data_collator\n",
        "    )\n",
        "\n",
        "    logger.info(\"***** chuRunning Prediction *****\")\n",
        "    logger.info(\"  Num examples = {}\".format(len(data_loader.dataset)))\n",
        "    logger.info(\"  Batch size = {}\".format(predict_args.batch_size))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with open(predict_args.input_file, \"r\") as f:\n",
        "        all_preds = []\n",
        "        for inputs in tqdm(data_loader, desc=\"Predicting\"):\n",
        "            for k, v in inputs.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    inputs[k] = v.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    token_type_ids=inputs['token_type_ids']\n",
        "                )\n",
        "                logits = outputs[0]\n",
        "\n",
        "            preds = model.decode(logits, inputs['decoder_mask'].bool())\n",
        "            preds_list = [[label_map[x] for x in seq] for seq in preds]\n",
        "\n",
        "            all_preds += preds_list\n",
        "\n",
        "    write_predictions(\n",
        "        predict_args.input_file,\n",
        "        predict_args.output_file,\n",
        "        all_preds\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "nvoO38lMD8kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import os\n",
        "\n",
        "from transformers import HfArgumentParser\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier.\"}\n",
        "    )\n",
        "    use_fast: bool = field(\n",
        "        default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_crf: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether using CRF for inference.\"}\n",
        "    )\n",
        "    use_cls: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether concatenating token representation with [CLS].\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExTrainingArguments(TrainingArguments):\n",
        "    crf_learning_rate: float = field(\n",
        "        default=5e-3, metadata={\"help\": \"The initial learning rate of CRF parameters for Adam.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "    data_dir: str = field(\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_train: str = field(\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_dev: str = field(\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_test: str = field(\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    labels: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    evaluate_during_training: bool = field(\n",
        "        default=False, metadata={\"help\": \"evaluate_during_training\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PredictArguments:\n",
        "    input_file: str = field(\n",
        "        metadata={\"help\": \"Path to a file containing sentences to be extracted (can be a single column file without labels).\"}\n",
        "    )\n",
        "    output_file: str = field(\n",
        "        default=\"output_file\",\n",
        "        metadata={\"help\": \"Path to a file saving the outputs.\"}\n",
        "    )\n",
        "    labels: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Path to a file containing all labels.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                  \"than this will be truncated, sequences shorter will be padded.\"},\n",
        "    )\n",
        "    batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size for prediction.\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached test data.\"}\n",
        "    )\n",
        "    no_cuda: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Do not use CUDA even when it is available.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_train_args(args):\n",
        "    parser = HfArgumentParser((ModelArguments, DataArguments, ExTrainingArguments))\n",
        "    if len(args) == 1 and args[0].endswith(\".json\"):\n",
        "        model_args, data_args, train_args = parser.parse_json_file(\n",
        "            json_file=os.path.abspath(args[0]))\n",
        "    else:\n",
        "        model_args, data_args, train_args = parser.parse_args_into_dataclasses(args=args)\n",
        "\n",
        "    return model_args, data_args, train_args\n",
        "\n",
        "\n",
        "def parse_predict_args(args):\n",
        "    parser = HfArgumentParser((ModelArguments, PredictArguments))\n",
        "    if len(args) == 1 and args[0].endswith(\".json\"):\n",
        "        model_args, predict_args = parser.parse_json_file(\n",
        "            json_file=os.path.abspath(args[0]))\n",
        "    else:\n",
        "        model_args, predict_args = parser.parse_args_into_dataclasses(args=args)\n",
        "\n",
        "    return model_args, predict_args\n",
        "\n"
      ],
      "metadata": {
        "id": "9VdamH1NECu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --no-cookies https://drive.google.com/uc?id=1--UFHJY3ldytNTsLcseIMDXEyzFeYEuj\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-oYW7JgHyhFj1HpbYvpHdsK8y-RwF2pW\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-KkDBrlyr3DF91BOTgp-vgCLwMe2JAmX\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=105JaXg7iAYF-0A7_srvSnaU0pfPE5ChH\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-6QHFiBYVWaNtbgCXdNbVfZ0WwdIB68N\n",
        "train = \"/content/train_optim_concat.txt\"\n",
        "dev = \"/content/dev_optim_concat.txt\"\n",
        "test = \"/content/test_optim_concat.txt\"\n",
        "prod_labels = \"/content/prod_labels.txt\"\n",
        "prod_train = \"/content/prod_train.json\""
      ],
      "metadata": {
        "id": "CnnYGs6OJR3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args, data_args, train_args = parse_train_args([prod_train])\n",
        "\n",
        "model_args.model_name_or_path = \"jiangg/chembert_cased\"\n",
        "data_args.data_dir = \"/content\"\n",
        "data_args.data_dir_train = train\n",
        "data_args.data_dir_dev = dev\n",
        "data_args.data_dir_test = test\n",
        "data_args.labels = prod_labels\n",
        "\n",
        "train(model_args, data_args, train_args)"
      ],
      "metadata": {
        "id": "cg_et9eHEFf9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}