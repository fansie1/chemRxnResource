{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ec1791818244fc0a3c4e36c830a2c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dfa6132a6c441efbd320ea37ec350f5",
              "IPY_MODEL_872acde3f9ea4d39ae93e6750370128b",
              "IPY_MODEL_5a00e2c733ab4881825ffe24bc07dedb"
            ],
            "layout": "IPY_MODEL_26f0da0b3bd3416891b8bf2b50f3690d"
          }
        },
        "0dfa6132a6c441efbd320ea37ec350f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0d0280d11024f0bb077c46d94c12691",
            "placeholder": "​",
            "style": "IPY_MODEL_416e5f0df7a643f9bcba8bd4e8991c39",
            "value": "Evaluation: 100%"
          }
        },
        "872acde3f9ea4d39ae93e6750370128b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bb16217488643d683d1f64904a62859",
            "max": 88,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c964833cdd0d453f9cfa6a84a4e55420",
            "value": 88
          }
        },
        "5a00e2c733ab4881825ffe24bc07dedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdce2346b9fc485aa69ccf2924d01819",
            "placeholder": "​",
            "style": "IPY_MODEL_645bd35ac6d14ac09bc3c5c23caa7d8e",
            "value": " 88/88 [00:13&lt;00:00,  6.82it/s]"
          }
        },
        "26f0da0b3bd3416891b8bf2b50f3690d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d0280d11024f0bb077c46d94c12691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "416e5f0df7a643f9bcba8bd4e8991c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bb16217488643d683d1f64904a62859": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c964833cdd0d453f9cfa6a84a4e55420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdce2346b9fc485aa69ccf2924d01819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "645bd35ac6d14ac09bc3c5c23caa7d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "510b115075db44c1956bb675a4332b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ebe07fa0fc547148da08adb692e2dd6",
              "IPY_MODEL_99d4bb3db43b4e279a86406eefd43707",
              "IPY_MODEL_cb58bdd39d65423cb91b40f8353bb92f"
            ],
            "layout": "IPY_MODEL_035f269dfec44f5c9924ab0aae90c04a"
          }
        },
        "2ebe07fa0fc547148da08adb692e2dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2213250e94394a48b7e2a0c411b4a689",
            "placeholder": "​",
            "style": "IPY_MODEL_23e5f12e8b744e888e258bd3941ff02c",
            "value": "Prediction: 100%"
          }
        },
        "99d4bb3db43b4e279a86406eefd43707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53a453b8e0764558a3cd6cc916d2c841",
            "max": 91,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bb1958cff1d43a8aa7c2974627c8eb8",
            "value": 91
          }
        },
        "cb58bdd39d65423cb91b40f8353bb92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac6a57196df34864ada6620fda7e2f14",
            "placeholder": "​",
            "style": "IPY_MODEL_52dbcf76e0064ff096645ecaf24b3c13",
            "value": " 91/91 [00:13&lt;00:00,  6.17it/s]"
          }
        },
        "035f269dfec44f5c9924ab0aae90c04a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2213250e94394a48b7e2a0c411b4a689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e5f12e8b744e888e258bd3941ff02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53a453b8e0764558a3cd6cc916d2c841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb1958cff1d43a8aa7c2974627c8eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac6a57196df34864ada6620fda7e2f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52dbcf76e0064ff096645ecaf24b3c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fansie1/chemRxnResource/blob/simple/chemrxnresource.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hACkvkomBz4i",
        "outputId": "b91ee9f0-5637-4060-eac5-a3455537b056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval\n",
        "!pip install transformers\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!pip install accelerate -U\n",
        "\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertForTokenClassification\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "class BertForTagging(BertForTokenClassification):\n",
        "    def __init__(self, config, use_cls=False):\n",
        "        super(BertForTagging, self).__init__(config)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        decoder_mask=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        active_loss = attention_mask.view(-1) == 1\n",
        "        active_logits = logits.view(-1, self.num_labels)\n",
        "        active_labels = torch.where(\n",
        "            active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "        )\n",
        "        loss = loss_fct(active_logits, active_labels)\n",
        "        outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "    def decode(self, logits, mask):\n",
        "        preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "        batch_size, seq_len = preds.shape\n",
        "        preds_list = [[] for _ in range(batch_size)]\n",
        "        for i in range(batch_size):\n",
        "            for j in range(seq_len):\n",
        "                if mask[i, j]:\n",
        "                    preds_list[i].append(preds[i,j])\n",
        "        return preds_list"
      ],
      "metadata": {
        "id": "G2JvApbnCGDf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from typing import Any, Callable, Optional\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from transformers import Trainer\n",
        "from transformers import PreTrainedModel\n",
        "# from transformers import is_wandb_available\n",
        "from transformers import TrainingArguments\n",
        "from transformers.data.data_collator import DataCollator\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class IETrainer(Trainer):\n",
        "    \"\"\"\n",
        "    IETrainer is inheritated from from transformers.Trainer, optimized for IE tasks.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        args: TrainingArguments,\n",
        "        data_collator: Optional[DataCollator] = None,\n",
        "        train_dataset: Optional[Dataset] = None,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        compute_metrics=None,\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,\n",
        "        use_crf: Optional[bool]=False,\n",
        "        epoch: int = 1\n",
        "    ):\n",
        "        super(IETrainer, self).__init__(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            optimizers=optimizers\n",
        "        )\n",
        "        self.use_crf = use_crf\n",
        "        self.epoch = epoch\n",
        "        self.global_step = None\n",
        "\n",
        "    def evaluate(self, eval_dataset: Optional[Dataset] = None) -> Dict:\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self._prediction_loop(eval_dataloader, description=\"Evaluation\")\n",
        "\n",
        "        return output\n",
        "\n",
        "    def predict(self, test_dataset: Dataset) -> Dict:\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "\n",
        "        return self._prediction_loop(test_dataloader, description=\"Prediction\")\n",
        "\n",
        "    def _prediction_loop(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        description: str\n",
        "    ) -> Dict:\n",
        "        model = self.model\n",
        "        batch_size = dataloader.batch_size\n",
        "\n",
        "        logger.info(\"***** Running %s *****\", description)\n",
        "        logger.info(\"  Num examples = %d\", self.num_examples(dataloader))\n",
        "        logger.info(\"  Batch size = %d\", batch_size)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        eval_losses: List[float] = []\n",
        "        preds_ids = []\n",
        "        label_ids = []\n",
        "        input_ids_list = []\n",
        "        for inputs in tqdm(dataloader, desc=description):\n",
        "            has_labels = any(\n",
        "                inputs.get(k) is not None\n",
        "                for k in [\"labels\", \"lm_labels\", \"masked_lm_labels\"]\n",
        "            )\n",
        "\n",
        "            for k, v in inputs.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    inputs[k] = v.to(self.args.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                if has_labels:\n",
        "                    step_eval_loss, logits = outputs[:2]\n",
        "                    eval_losses += [step_eval_loss.mean().item()]\n",
        "                else:\n",
        "                    logits = outputs[0]\n",
        "\n",
        "            mask = inputs[\"decoder_mask\"].to(torch.bool)\n",
        "            preds = model.decode(logits, mask=mask)\n",
        "            preds_ids.extend(preds)\n",
        "            input_ids_list.extend(inputs[\"input_ids\"])\n",
        "            if inputs.get(\"labels\") is not None:\n",
        "                labels = [inputs[\"labels\"][i, mask[i]].tolist() \\\n",
        "                            for i in range(inputs[\"labels\"].shape[0])]\n",
        "                label_ids.extend(labels)\n",
        "                assert len(preds) == len(labels)\n",
        "                assert len(preds[0]) == len(labels[0])\n",
        "\n",
        "        if self.compute_metrics is not None and \\\n",
        "                len(preds_ids) > 0 and \\\n",
        "                len(label_ids) > 0:\n",
        "            metrics = self.compute_metrics(preds_ids, label_ids)\n",
        "        else:\n",
        "            metrics = {}\n",
        "        if len(eval_losses) > 0:\n",
        "            metrics['eval_loss'] = np.mean(eval_losses)\n",
        "\n",
        "        # Prefix all keys with eval_\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(\"eval_\"):\n",
        "                metrics[f\"eval_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return {'predictions': preds_ids, 'label_ids': label_ids, 'metrics': metrics, \"input_ids_list\": input_ids_list}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXJNaHwkCtOv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnCHIaXJ00GM",
        "outputId": "15f51824-0720-41ff-ff51-0e2cf7f6c224"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from typing import List, Optional, Union\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class InputExample:\n",
        "    \"\"\"\n",
        "    A single training/test example for token classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        words: list. The words of the sequence.\n",
        "        labels: (Optional) list. The labels for each word of the sequence. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    guid: str\n",
        "    words: List[str]\n",
        "    metainfo: Optional[str] = None\n",
        "    labels: Optional[List[str]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class InputFeatures:\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    Property names are the same names as the corresponding inputs to a model.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids: List[int]\n",
        "    attention_mask: List[int]\n",
        "    token_type_ids: Optional[List[int]] = None\n",
        "    label_ids: Optional[List[int]] = None\n",
        "    decoder_mask: Optional[List[bool]] = None\n",
        "\n",
        "class ProdDataset(Dataset):\n",
        "    features: List[InputFeatures]\n",
        "    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n",
        "    # Use cross entropy ignore_index as padding label id so that only\n",
        "    # real label ids contribute to the loss later.\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_file: str,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        labels: List[str],\n",
        "        model_type: str,\n",
        "        max_seq_length: Optional[int] = None,\n",
        "        overwrite_cache=False\n",
        "    ):\n",
        "        # Load data features from cache or dataset file\n",
        "        data_dir = os.path.dirname(data_file)\n",
        "        fname = os.path.basename(data_file)\n",
        "        cached_features_file = os.path.join(\n",
        "            \"/content/\",\n",
        "            \"cached_{}_{}_{}\".format(\n",
        "                fname,\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(max_seq_length)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "            logger.info(f\"Loading features from cached file {cached_features_file}\")\n",
        "            self.features = torch.load(cached_features_file)\n",
        "        else:\n",
        "            logger.info(f\"Creating features from dataset file at {data_file}\")\n",
        "            examples = read_examples_from_file(data_file)\n",
        "            self.features = convert_examples_to_features(\n",
        "                examples,\n",
        "                labels,\n",
        "                max_seq_length,\n",
        "                tokenizer,\n",
        "                cls_token=tokenizer.cls_token,\n",
        "                cls_token_segment_id=0,\n",
        "                sep_token=tokenizer.sep_token,\n",
        "                pad_token=tokenizer.pad_token_id,\n",
        "                pad_token_segment_id=tokenizer.pad_token_type_id,\n",
        "                pad_token_label_id=self.pad_token_label_id,\n",
        "            )\n",
        "            logger.info(f\"Saving features into cached file {cached_features_file}\")\n",
        "            torch.save(self.features, cached_features_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, i) -> InputFeatures:\n",
        "        return self.features[i]\n",
        "\n",
        "def read_examples_from_file(file_path) -> List[InputExample]:\n",
        "    guid_index = 1\n",
        "    examples = []\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        words, labels = [], []\n",
        "        metainfo = None\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line.startswith(\"#\\tpassage\"):\n",
        "                metainfo = line\n",
        "            elif line == \"\":\n",
        "                if words:\n",
        "                    examples.append(InputExample(\n",
        "                        guid=f\"{guid_index}\",\n",
        "                        words=words,\n",
        "                        metainfo=metainfo,\n",
        "                        labels=labels\n",
        "                    ))\n",
        "                    guid_index += 1\n",
        "                    words, labels = [], []\n",
        "            else:\n",
        "                splits = line.split(\"\\t\")\n",
        "                words.append(splits[0])\n",
        "                if len(splits) > 1:\n",
        "                    labels.append(splits[-1])\n",
        "                else:\n",
        "                    # Examples could have no label for plain test files\n",
        "                    labels.append(\"O\")\n",
        "        if words:\n",
        "            examples.append(InputExample(\n",
        "                guid=f\"{guid_index}\",\n",
        "                words=words,\n",
        "                metainfo=metainfo,\n",
        "                labels=labels\n",
        "            ))\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples: List[InputExample],\n",
        "    label_list: List[str],\n",
        "    max_seq_length: int,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    cls_token=\"[CLS]\",\n",
        "    cls_token_segment_id=0,\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    pad_token_label_id=-100,\n",
        "    sequence_a_segment_id=0,\n",
        "    sequence_b_segment_id=1,\n",
        "    mask_padding_with_zero=True,\n",
        "    verbose=False\n",
        ") -> List[InputFeatures]:\n",
        "    \"\"\" Loads a data file into a list of `InputFeatures`\n",
        "    \"\"\"\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10_000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
        "\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "        for word, label in zip(example.words, example.labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            # word_tokens = word_tokens[:5]\n",
        "\n",
        "            if len(word_tokens) > 0:\n",
        "                tokens.extend(word_tokens)\n",
        "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        if len(tokens) > max_seq_length - 2:\n",
        "            logger.warning(\"Sequence length exceed {} (cut).\".format(max_seq_length))\n",
        "            tokens = tokens[: (max_seq_length - 2)]\n",
        "            label_ids = label_ids[: (max_seq_length - 2)]\n",
        "\n",
        "        tokens += [sep_token]\n",
        "        label_ids += [pad_token_label_id]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_ids = [pad_token_label_id] + label_ids\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        seq_length = len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        input_ids += [pad_token] * padding_length\n",
        "        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
        "        segment_ids += [pad_token_segment_id] * padding_length\n",
        "        label_ids += [pad_token_label_id] * padding_length\n",
        "\n",
        "        decoder_mask = [(x != pad_token_label_id) for x in label_ids]\n",
        "\n",
        "        # assert len(input_ids) == max_seq_length\n",
        "        # assert len(input_mask) == max_seq_length\n",
        "        # assert len(segment_ids) == max_seq_length\n",
        "        # assert len(label_ids) == max_seq_length\n",
        "\n",
        "        if verbose and ex_index < 1:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: {} (length: {})\".format(example.guid, seq_length))\n",
        "            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens[:seq_length]]))\n",
        "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids[:seq_length]]))\n",
        "            # logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
        "            # logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids[:seq_length]]))\n",
        "            logger.info(\"decode_mask: %s\", \" \".join([str(x) for x in decoder_mask[:seq_length]]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=input_mask,\n",
        "                token_type_ids=segment_ids,\n",
        "                label_ids=label_ids,\n",
        "                decoder_mask=decoder_mask\n",
        "            )\n",
        "        )\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "vivrNVsuCuxn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_labels(path: str) -> List[str]:\n",
        "    if path:\n",
        "        with open(path, \"r\") as f:\n",
        "            labels = f.read().splitlines()\n",
        "        if \"O\" not in labels:\n",
        "            labels = [\"O\"] + labels\n",
        "        return labels\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def write_predictions(input_file, output_file, predictions):\n",
        "    \"\"\" Write Product Extraction predictions to file,\n",
        "        while aligning with the input format.\n",
        "    \"\"\"\n",
        "    with open(output_file, \"w\") as writer, open(input_file, \"r\") as f:\n",
        "        example_id = 0\n",
        "        for line in f:\n",
        "            if line.startswith(\"#\\tpassage\"):\n",
        "                writer.write(line)\n",
        "            elif line == \"\" or line == \"\\n\":\n",
        "                writer.write(line)\n",
        "                if not predictions[example_id]:\n",
        "                    example_id += 1\n",
        "            elif predictions[example_id]:\n",
        "                cols = line.rstrip().split()\n",
        "                cols.append(predictions[example_id].pop(0))\n",
        "                writer.write(\"\\t\".join(cols) + \"\\n\")\n",
        "            else:\n",
        "                logger.warning(\n",
        "                    \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
        "                )\n",
        "\n",
        "def write_result(output, predictions, labels):\n",
        "  predictions = [j for i in predictions for j in i]\n",
        "  labels = [j for i in labels for j in i]\n",
        "  with open(output, \"w\") as writer:\n",
        "    for p, l in zip(predictions, labels):\n",
        "      writer.write(str(l) + \"\\t\" + str(p) + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "39T42L5QQL-s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "from transformers import set_seed\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def train(model_args, data_args, train_args):\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(train_args.seed)\n",
        "\n",
        "    # Prepare prod-ext task\n",
        "    labels = get_labels(data_args.labels)\n",
        "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        id2label=label_map,\n",
        "        label2id={label: i for i, label in enumerate(labels)},\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast,\n",
        "    )\n",
        "    model = BertForTagging.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_cls=model_args.use_cls\n",
        "    )\n",
        "\n",
        "    # Get datasets\n",
        "    # pdb.set_trace()\n",
        "    train_dataset = (\n",
        "        ProdDataset(\n",
        "            # data_file=os.path.join(data_args.data_dir, \"train.txt\"),\n",
        "            data_file=data_args.data_dir_train,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache\n",
        "        )\n",
        "        if train_args.do_train\n",
        "        else None\n",
        "    )\n",
        "    eval_dataset = (\n",
        "        ProdDataset(\n",
        "            # data_file=os.path.join(data_args.data_dir, \"dev.txt\"),\n",
        "            data_file=data_args.data_dir_dev,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache\n",
        "        )\n",
        "        if train_args.do_eval\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    def compute_metrics(predictions, label_ids) -> Dict:\n",
        "        label_list = [[label_map[x] for x in seq] for seq in label_ids]\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "\n",
        "        return {\n",
        "            \"precision\": precision_score(label_list, preds_list),\n",
        "            \"recall\": recall_score(label_list, preds_list),\n",
        "            \"f1\": f1_score(label_list, preds_list),\n",
        "        }\n",
        "\n",
        "    metrics_fn = compute_metrics\n",
        "    dataset_len = len(train_dataset) if train_args.do_train else 0\n",
        "    batch_size = train_args.per_device_train_batch_size\n",
        "    total_steps = (dataset_len // batch_size) * train_args.num_train_epochs if dataset_len % batch_size == 0 else \\\n",
        "        (dataset_len // batch_size + 1) * train_args.num_train_epochs\n",
        "    train_args.warmup_steps = 0.1 * total_steps\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = IETrainer(\n",
        "        model=model,\n",
        "        args=train_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=metrics_fn,\n",
        "        use_crf=model_args.use_crf,\n",
        "        optimizers=get_optimizer_grouped_parameters(\n",
        "            args=train_args,\n",
        "            model=model,\n",
        "            num_training_steps=total_steps),\n",
        "        epoch=train_args.num_train_epochs\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if train_args.do_train:\n",
        "        trainer.train()\n",
        "        # trainer.save_model()\n",
        "        # tokenizer.save_pretrained(train_args.output_dir)\n",
        "\n",
        "    # Evaluation\n",
        "    if train_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        output = trainer.evaluate()\n",
        "        predictions = output['predictions']\n",
        "        label_ids = output['label_ids']\n",
        "        metrics = output['metrics']\n",
        "        output_eval_file = os.path.join(train_args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            for key, value in metrics.items():\n",
        "                logger.info(\"  %s = %s\", key, value)\n",
        "                writer.write(\"%s = %s\\n\" % (key, value))\n",
        "\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "        print(\"----------dev----------\")\n",
        "        print(metrics)\n",
        "        write_result(\"inputs_result.txt\", output[\"input_ids_list\"], output[\"label_ids\"])\n",
        "        # Save predictions\n",
        "        write_predictions(\n",
        "            data_args.data_dir_dev,\n",
        "            os.path.join(train_args.output_dir, \"eval_predictions.txt\"),\n",
        "            preds_list\n",
        "        )\n",
        "\n",
        "    # Predict\n",
        "    if train_args.do_predict:\n",
        "        test_dataset = ProdDataset(\n",
        "            data_file=data_args.data_dir_test,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache,\n",
        "        )\n",
        "\n",
        "        output = trainer.predict(test_dataset)\n",
        "\n",
        "        predictions = output['predictions']\n",
        "        label_ids = output['label_ids']\n",
        "        metrics = output['metrics']\n",
        "\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "        print(\"----------test----------\")\n",
        "        print(metrics)\n",
        "        output_test_results_file = os.path.join(train_args.output_dir, \"test_results.txt\")\n",
        "        with open(output_test_results_file, \"w\") as writer:\n",
        "            for key, value in metrics.items():\n",
        "                logger.info(\"  %s = %s\", key, value)\n",
        "                writer.write(\"%s = %s\\n\" % (key, value))\n",
        "\n",
        "        # Save predictions\n",
        "        write_predictions(\n",
        "            data_args.data_dir_test,\n",
        "            os.path.join(train_args.output_dir, \"test_predictions.txt\"),\n",
        "            preds_list\n",
        "        )\n",
        "\n",
        "def get_optimizer_grouped_parameters(args, model, num_training_steps):\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in model.named_parameters()\n",
        "                if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in model.named_parameters()\n",
        "                if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=args.learning_rate,\n",
        "        eps=args.adam_epsilon\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=args.warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def predict(model_args, predict_args):\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # logger = create_logger(name=\"predict_prod\", save_dir=train_args.output_dir)\n",
        "    logger.info(\"Predict parameters %s\", predict_args)\n",
        "\n",
        "    # Prepare prod-ext task\n",
        "    labels = get_labels(predict_args.labels)\n",
        "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        id2label=label_map,\n",
        "        label2id={label: i for i, label in enumerate(labels)},\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast,\n",
        "    )\n",
        "    model = BertForTagging.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir\n",
        "    )\n",
        "\n",
        "    device = torch.device(\n",
        "                \"cuda\"\n",
        "                if (not predict_args.no_cuda and torch.cuda.is_available())\n",
        "                else \"cpu\"\n",
        "            )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # load test dataset\n",
        "    test_dataset = ProdDataset(\n",
        "        data_file=predict_args.input_file,\n",
        "        tokenizer=tokenizer,\n",
        "        labels=labels,\n",
        "        model_type=config.model_type,\n",
        "        max_seq_length=predict_args.max_seq_length,\n",
        "        overwrite_cache=predict_args.overwrite_cache,\n",
        "    )\n",
        "\n",
        "    sampler = SequentialSampler(test_dataset)\n",
        "    data_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=sampler,\n",
        "        batch_size=predict_args.batch_size,\n",
        "        collate_fn=default_data_collator\n",
        "    )\n",
        "\n",
        "    logger.info(\"***** chuRunning Prediction *****\")\n",
        "    logger.info(\"  Num examples = {}\".format(len(data_loader.dataset)))\n",
        "    logger.info(\"  Batch size = {}\".format(predict_args.batch_size))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with open(predict_args.input_file, \"r\") as f:\n",
        "        all_preds = []\n",
        "        for inputs in tqdm(data_loader, desc=\"Predicting\"):\n",
        "            for k, v in inputs.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    inputs[k] = v.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    token_type_ids=inputs['token_type_ids']\n",
        "                )\n",
        "                logits = outputs[0]\n",
        "\n",
        "            preds = model.decode(logits, inputs['decoder_mask'].bool())\n",
        "            preds_list = [[label_map[x] for x in seq] for seq in preds]\n",
        "\n",
        "            all_preds += preds_list\n",
        "\n",
        "    write_predictions(\n",
        "        predict_args.input_file,\n",
        "        predict_args.output_file,\n",
        "        all_preds\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "nvoO38lMD8kV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import os\n",
        "\n",
        "from transformers import HfArgumentParser\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier.\"}\n",
        "    )\n",
        "    use_fast: bool = field(\n",
        "        default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_crf: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether using CRF for inference.\"}\n",
        "    )\n",
        "    use_cls: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether concatenating token representation with [CLS].\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExTrainingArguments(TrainingArguments):\n",
        "    crf_learning_rate: float = field(\n",
        "        default=5e-3, metadata={\"help\": \"The initial learning rate of CRF parameters for Adam.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "    data_dir: str = field(\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_train: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_dev: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_test: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    labels: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    evaluate_during_training: bool = field(\n",
        "        default=False, metadata={\"help\": \"evaluate_during_training\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PredictArguments:\n",
        "    input_file: str = field(\n",
        "        metadata={\"help\": \"Path to a file containing sentences to be extracted (can be a single column file without labels).\"}\n",
        "    )\n",
        "    output_file: str = field(\n",
        "        default=\"/content\",\n",
        "        metadata={\"help\": \"Path to a file saving the outputs.\"}\n",
        "    )\n",
        "    labels: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Path to a file containing all labels.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                  \"than this will be truncated, sequences shorter will be padded.\"},\n",
        "    )\n",
        "    batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size for prediction.\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached test data.\"}\n",
        "    )\n",
        "    no_cuda: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Do not use CUDA even when it is available.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_train_args(args):\n",
        "    parser = HfArgumentParser((ModelArguments, DataArguments, ExTrainingArguments))\n",
        "    if len(args) == 1 and args[0].endswith(\".json\"):\n",
        "        model_args, data_args, train_args = parser.parse_json_file(\n",
        "            json_file=os.path.abspath(args[0]))\n",
        "    else:\n",
        "        model_args, data_args, train_args = parser.parse_args_into_dataclasses(args=args)\n",
        "\n",
        "    return model_args, data_args, train_args\n",
        "\n",
        "\n",
        "def parse_predict_args(args):\n",
        "    parser = HfArgumentParser((ModelArguments, PredictArguments))\n",
        "    if len(args) == 1 and args[0].endswith(\".json\"):\n",
        "        model_args, predict_args = parser.parse_json_file(\n",
        "            json_file=os.path.abspath(args[0]))\n",
        "    else:\n",
        "        model_args, predict_args = parser.parse_args_into_dataclasses(args=args)\n",
        "\n",
        "    return model_args, predict_args\n",
        "\n"
      ],
      "metadata": {
        "id": "9VdamH1NECu9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --no-cookies https://drive.google.com/uc?id=1--UFHJY3ldytNTsLcseIMDXEyzFeYEuj\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-oYW7JgHyhFj1HpbYvpHdsK8y-RwF2pW\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-KkDBrlyr3DF91BOTgp-vgCLwMe2JAmX\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=105JaXg7iAYF-0A7_srvSnaU0pfPE5ChH\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-6QHFiBYVWaNtbgCXdNbVfZ0WwdIB68N\n",
        "train_data = \"/content/train_optim_concat.txt\"\n",
        "dev_data = \"/content/dev_optim_concat.txt\"\n",
        "test_data = \"/content/test_optim_concat.txt\"\n",
        "prod_labels = \"/content/prod_labels.txt\"\n",
        "prod_train = \"/content/prod_train.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnnYGs6OJR3o",
        "outputId": "ae616a39-3978-438b-f6b7-b9317223c4bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--UFHJY3ldytNTsLcseIMDXEyzFeYEuj\n",
            "To: /content/train_optim_concat.txt\n",
            "100% 1.57M/1.57M [00:00<00:00, 49.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-oYW7JgHyhFj1HpbYvpHdsK8y-RwF2pW\n",
            "To: /content/test_optim_concat.txt\n",
            "100% 167k/167k [00:00<00:00, 88.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-KkDBrlyr3DF91BOTgp-vgCLwMe2JAmX\n",
            "To: /content/dev_optim_concat.txt\n",
            "100% 162k/162k [00:00<00:00, 95.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=105JaXg7iAYF-0A7_srvSnaU0pfPE5ChH\n",
            "To: /content/prod_labels.txt\n",
            "100% 19.0/19.0 [00:00<00:00, 74.5kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-6QHFiBYVWaNtbgCXdNbVfZ0WwdIB68N\n",
            "To: /content/prod_train.json\n",
            "100% 504/504 [00:00<00:00, 1.78MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_args, data_args, train_args = parse_train_args([prod_train])\n",
        "\n",
        "model_args.model_name_or_path = \"jiangg/chembert_cased\"\n",
        "data_args.data_dir = \"/content\"\n",
        "data_args.data_dir_train = train_data\n",
        "data_args.data_dir_dev = dev_data\n",
        "data_args.data_dir_test = test_data\n",
        "data_args.labels = prod_labels\n",
        "train_args.per_device_train_batch_size = 32\n",
        "train_args.output_dir = \"/content/drive/MyDrive/chemrxnconfig/models\"\n",
        "# train_args.do_train = False\n",
        "# pdb.set_trace()\n",
        "train(model_args, data_args, train_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342,
          "referenced_widgets": [
            "4ec1791818244fc0a3c4e36c830a2c3a",
            "0dfa6132a6c441efbd320ea37ec350f5",
            "872acde3f9ea4d39ae93e6750370128b",
            "5a00e2c733ab4881825ffe24bc07dedb",
            "26f0da0b3bd3416891b8bf2b50f3690d",
            "e0d0280d11024f0bb077c46d94c12691",
            "416e5f0df7a643f9bcba8bd4e8991c39",
            "8bb16217488643d683d1f64904a62859",
            "c964833cdd0d453f9cfa6a84a4e55420",
            "fdce2346b9fc485aa69ccf2924d01819",
            "645bd35ac6d14ac09bc3c5c23caa7d8e",
            "510b115075db44c1956bb675a4332b96",
            "2ebe07fa0fc547148da08adb692e2dd6",
            "99d4bb3db43b4e279a86406eefd43707",
            "cb58bdd39d65423cb91b40f8353bb92f",
            "035f269dfec44f5c9924ab0aae90c04a",
            "2213250e94394a48b7e2a0c411b4a689",
            "23e5f12e8b744e888e258bd3941ff02c",
            "53a453b8e0764558a3cd6cc916d2c841",
            "0bb1958cff1d43a8aa7c2974627c8eb8",
            "ac6a57196df34864ada6620fda7e2f14",
            "52dbcf76e0064ff096645ecaf24b3c13"
          ]
        },
        "id": "cg_et9eHEFf9",
        "outputId": "d89460d6-355d-4a18-cde6-6eed9f7a8f8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTagging were not initialized from the model checkpoint at jiangg/chembert_cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:__main__:Sequence length exceed 256 (cut).\n",
            "WARNING:__main__:Sequence length exceed 256 (cut).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='193' max='193' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [193/193 04:05, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation:   0%|          | 0/88 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ec1791818244fc0a3c4e36c830a2c3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------dev----------\n",
            "{'eval_loss': 0.016063255867596705, 'eval_precision': 0.4306569343065693, 'eval_recall': 0.6145833333333334, 'eval_f1': 0.5064377682403434}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Maximum sequence length exceeded: No prediction for '.'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Prediction:   0%|          | 0/91 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "510b115075db44c1956bb675a4332b96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------test----------\n",
            "{'eval_loss': 0.012970137373166246, 'eval_precision': 0.7192982456140351, 'eval_recall': 0.7387387387387387, 'eval_f1': 0.7288888888888888}\n"
          ]
        }
      ]
    }
  ]
}