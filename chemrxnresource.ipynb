{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fansie1/chemRxnResource/blob/main/chemrxnresource.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hACkvkomBz4i",
        "outputId": "2fb774da-79ce-4f6d-8c13-39d52680c129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval\n",
        "!pip install transformers\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!pip install accelerate -U\n",
        "\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertForTokenClassification\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "class BertForTagging(BertForTokenClassification):\n",
        "    def __init__(self, config, use_cls=False):\n",
        "        super(BertForTagging, self).__init__(config)\n",
        "\n",
        "        self.use_cls = use_cls\n",
        "        if self.use_cls:\n",
        "            self.classifier = nn.Linear(config.hidden_size * 2, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "#         pdb.set_trace()\n",
        "        if self.use_cls:\n",
        "            batch_size, seq_length, hidden_dim = sequence_output.shape\n",
        "            extended_cls_h = outputs[1].unsqueeze(1).expand(batch_size, seq_length, hidden_dim)\n",
        "            sequence_output = torch.cat([sequence_output, extended_cls_h], 2)\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "    def decode(self, logits, mask):\n",
        "        preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "        batch_size, seq_len = preds.shape\n",
        "        preds_list = [[] for _ in range(batch_size)]\n",
        "        for i in range(batch_size):\n",
        "            for j in range(seq_len):\n",
        "                if mask[i, j]:\n",
        "                    preds_list[i].append(preds[i,j])\n",
        "        return preds_list"
      ],
      "metadata": {
        "id": "G2JvApbnCGDf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# шонч╗Г\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from typing import Any, Callable, Optional\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from transformers import Trainer\n",
        "from transformers import PreTrainedModel\n",
        "# from transformers import is_wandb_available\n",
        "from transformers import TrainingArguments\n",
        "from transformers.data.data_collator import DataCollator\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class IETrainer(Trainer):\n",
        "    \"\"\"\n",
        "    IETrainer is inheritated from from transformers.Trainer, optimized for IE tasks.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        args: TrainingArguments,\n",
        "        data_collator: Optional[DataCollator] = None,\n",
        "        train_dataset: Optional[Dataset] = None,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        compute_metrics=None,\n",
        "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = None,\n",
        "        use_crf: Optional[bool]=False,\n",
        "        epoch: int = 1\n",
        "    ):\n",
        "        super(IETrainer, self).__init__(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            optimizers=optimizers\n",
        "        )\n",
        "        self.use_crf = use_crf\n",
        "        self.epoch = epoch\n",
        "        self.global_step = None\n",
        "\n",
        "    def evaluate(self, eval_dataset: Optional[Dataset] = None) -> Dict:\n",
        "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        output = self._prediction_loop(eval_dataloader, description=\"Evaluation\")\n",
        "\n",
        "        self._log(output['metrics'])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def predict(self, test_dataset: Dataset) -> Dict:\n",
        "        test_dataloader = self.get_test_dataloader(test_dataset)\n",
        "\n",
        "        return self._prediction_loop(test_dataloader, description=\"Prediction\")\n",
        "\n",
        "    def _prediction_loop(\n",
        "        self,\n",
        "        dataloader: DataLoader,\n",
        "        description: str\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Prediction/evaluation loop, shared by `evaluate()` and `predict()`\n",
        "        Works both with or without labels.\n",
        "        \"\"\"\n",
        "        model = self.model\n",
        "        batch_size = dataloader.batch_size\n",
        "\n",
        "        logger.info(\"***** Running %s *****\", description)\n",
        "        logger.info(\"  Num examples = %d\", self.num_examples(dataloader))\n",
        "        logger.info(\"  Batch size = %d\", batch_size)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        eval_losses: List[float] = []\n",
        "        preds_ids = []\n",
        "        label_ids = []\n",
        "\n",
        "        for inputs in tqdm(dataloader, desc=description):\n",
        "            has_labels = any(\n",
        "                inputs.get(k) is not None\n",
        "                for k in [\"labels\", \"lm_labels\", \"masked_lm_labels\"]\n",
        "            )\n",
        "\n",
        "            for k, v in inputs.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    inputs[k] = v.to(self.args.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                if has_labels:\n",
        "                    step_eval_loss, logits = outputs[:2]\n",
        "                    eval_losses += [step_eval_loss.mean().item()]\n",
        "                else:\n",
        "                    logits = outputs[0]\n",
        "\n",
        "            mask = inputs[\"decoder_mask\"].to(torch.bool)\n",
        "            preds = model.decode(logits, mask=mask)\n",
        "            preds_ids.extend(preds)\n",
        "            if inputs.get(\"labels\") is not None:\n",
        "                labels = [inputs[\"labels\"][i, mask[i]].tolist() \\\n",
        "                            for i in range(inputs[\"labels\"].shape[0])]\n",
        "                label_ids.extend(labels)\n",
        "                assert len(preds) == len(labels)\n",
        "                assert len(preds[0]) == len(labels[0])\n",
        "\n",
        "        if self.compute_metrics is not None and \\\n",
        "                len(preds_ids) > 0 and \\\n",
        "                len(label_ids) > 0:\n",
        "            metrics = self.compute_metrics(preds_ids, label_ids)\n",
        "        else:\n",
        "            metrics = {}\n",
        "        if len(eval_losses) > 0:\n",
        "            metrics['eval_loss'] = np.mean(eval_losses)\n",
        "\n",
        "        # Prefix all keys with eval_\n",
        "        for key in list(metrics.keys()):\n",
        "            if not key.startswith(\"eval_\"):\n",
        "                metrics[f\"eval_{key}\"] = metrics.pop(key)\n",
        "\n",
        "        return {'predictions': preds_ids, 'label_ids': label_ids, 'metrics': metrics}\n",
        "\n",
        "    def _log(self, logs: Dict[str, float], iterator: Optional[tqdm] = None) -> None:\n",
        "        if self.epoch is not None:\n",
        "            logs[\"epoch\"] = self.epoch\n",
        "        if self.global_step is None:\n",
        "            # when logging evaluation metrics without training\n",
        "            self.global_step = 0\n",
        "        # if is_wandb_available():\n",
        "        #     if self.is_world_master():\n",
        "        #         wandb.log(logs, step=self.global_step)\n",
        "        output = {**logs, **{\"step\": self.global_step}}\n",
        "        if iterator is not None:\n",
        "            iterator.write(output)\n",
        "        else:\n",
        "            logger.info(\n",
        "                {k:round(v, 4) if isinstance(v, float) else v for k, v in output.items()}\n",
        "            )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXJNaHwkCtOv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from typing import List, Optional, Union\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class InputExample:\n",
        "    \"\"\"\n",
        "    A single training/test example for token classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        words: list. The words of the sequence.\n",
        "        labels: (Optional) list. The labels for each word of the sequence. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    guid: str\n",
        "    words: List[str]\n",
        "    metainfo: Optional[str] = None\n",
        "    labels: Optional[List[str]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class InputFeatures:\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    Property names are the same names as the corresponding inputs to a model.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids: List[int]\n",
        "    attention_mask: List[int]\n",
        "    token_type_ids: Optional[List[int]] = None\n",
        "    label_ids: Optional[List[int]] = None\n",
        "    decoder_mask: Optional[List[bool]] = None\n",
        "\n",
        "class ProdDataset(Dataset):\n",
        "    features: List[InputFeatures]\n",
        "    pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n",
        "    # Use cross entropy ignore_index as padding label id so that only\n",
        "    # real label ids contribute to the loss later.\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_file: str,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        labels: List[str],\n",
        "        model_type: str,\n",
        "        max_seq_length: Optional[int] = None,\n",
        "        overwrite_cache=False\n",
        "    ):\n",
        "        # Load data features from cache or dataset file\n",
        "        data_dir = os.path.dirname(data_file)\n",
        "        fname = os.path.basename(data_file)\n",
        "        cached_features_file = os.path.join(\n",
        "            \"/content/\",\n",
        "            \"cached_{}_{}_{}\".format(\n",
        "                fname,\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(max_seq_length)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "            logger.info(f\"Loading features from cached file {cached_features_file}\")\n",
        "            self.features = torch.load(cached_features_file)\n",
        "        else:\n",
        "            logger.info(f\"Creating features from dataset file at {data_file}\")\n",
        "            examples = read_examples_from_file(data_file)\n",
        "            self.features = convert_examples_to_features(\n",
        "                examples,\n",
        "                labels,\n",
        "                max_seq_length,\n",
        "                tokenizer,\n",
        "                cls_token=tokenizer.cls_token,\n",
        "                cls_token_segment_id=0,\n",
        "                sep_token=tokenizer.sep_token,\n",
        "                pad_token=tokenizer.pad_token_id,\n",
        "                pad_token_segment_id=tokenizer.pad_token_type_id,\n",
        "                pad_token_label_id=self.pad_token_label_id,\n",
        "            )\n",
        "            logger.info(f\"Saving features into cached file {cached_features_file}\")\n",
        "            torch.save(self.features, cached_features_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, i) -> InputFeatures:\n",
        "        return self.features[i]\n",
        "\n",
        "def read_examples_from_file(file_path) -> List[InputExample]:\n",
        "    guid_index = 1\n",
        "    examples = []\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        words, labels = [], []\n",
        "        metainfo = None\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line.startswith(\"#\\tpassage\"):\n",
        "                metainfo = line\n",
        "            elif line == \"\":\n",
        "                if words:\n",
        "                    examples.append(InputExample(\n",
        "                        guid=f\"{guid_index}\",\n",
        "                        words=words,\n",
        "                        metainfo=metainfo,\n",
        "                        labels=labels\n",
        "                    ))\n",
        "                    guid_index += 1\n",
        "                    words, labels = [], []\n",
        "            else:\n",
        "                splits = line.split(\"\\t\")\n",
        "                words.append(splits[0])\n",
        "                if len(splits) > 1:\n",
        "                    labels.append(splits[-1])\n",
        "                else:\n",
        "                    # Examples could have no label for plain test files\n",
        "                    labels.append(\"O\")\n",
        "        if words:\n",
        "            examples.append(InputExample(\n",
        "                guid=f\"{guid_index}\",\n",
        "                words=words,\n",
        "                metainfo=metainfo,\n",
        "                labels=labels\n",
        "            ))\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples: List[InputExample],\n",
        "    label_list: List[str],\n",
        "    max_seq_length: int,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    cls_token=\"[CLS]\",\n",
        "    cls_token_segment_id=0,\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    pad_token_label_id=-100,\n",
        "    sequence_a_segment_id=0,\n",
        "    sequence_b_segment_id=1,\n",
        "    mask_padding_with_zero=True,\n",
        "    verbose=False\n",
        ") -> List[InputFeatures]:\n",
        "    \"\"\" Loads a data file into a list of `InputFeatures`\n",
        "    \"\"\"\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10_000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
        "\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "        for word, label in zip(example.words, example.labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            # word_tokens = word_tokens[:5]\n",
        "\n",
        "            if len(word_tokens) > 0:\n",
        "                tokens.extend(word_tokens)\n",
        "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        if len(tokens) > max_seq_length - 2:\n",
        "            logger.warning(\"Sequence length exceed {} (cut).\".format(max_seq_length))\n",
        "            tokens = tokens[: (max_seq_length - 2)]\n",
        "            label_ids = label_ids[: (max_seq_length - 2)]\n",
        "\n",
        "        tokens += [sep_token]\n",
        "        label_ids += [pad_token_label_id]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_ids = [pad_token_label_id] + label_ids\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        seq_length = len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        input_ids += [pad_token] * padding_length\n",
        "        input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
        "        segment_ids += [pad_token_segment_id] * padding_length\n",
        "        label_ids += [pad_token_label_id] * padding_length\n",
        "\n",
        "        decoder_mask = [(x != pad_token_label_id) for x in label_ids]\n",
        "\n",
        "        # assert len(input_ids) == max_seq_length\n",
        "        # assert len(input_mask) == max_seq_length\n",
        "        # assert len(segment_ids) == max_seq_length\n",
        "        # assert len(label_ids) == max_seq_length\n",
        "\n",
        "        if verbose and ex_index < 1:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: {} (length: {})\".format(example.guid, seq_length))\n",
        "            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens[:seq_length]]))\n",
        "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids[:seq_length]]))\n",
        "            # logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
        "            # logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids[:seq_length]]))\n",
        "            logger.info(\"decode_mask: %s\", \" \".join([str(x) for x in decoder_mask[:seq_length]]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=input_mask,\n",
        "                token_type_ids=segment_ids,\n",
        "                label_ids=label_ids,\n",
        "                decoder_mask=decoder_mask\n",
        "            )\n",
        "        )\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "vivrNVsuCuxn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_labels(path: str) -> List[str]:\n",
        "    if path:\n",
        "        with open(path, \"r\") as f:\n",
        "            labels = f.read().splitlines()\n",
        "        if \"O\" not in labels:\n",
        "            labels = [\"O\"] + labels\n",
        "        return labels\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def write_predictions(input_file, output_file, predictions):\n",
        "    \"\"\" Write Product Extraction predictions to file,\n",
        "        while aligning with the input format.\n",
        "    \"\"\"\n",
        "    with open(output_file, \"w\") as writer, open(input_file, \"r\") as f:\n",
        "        example_id = 0\n",
        "        for line in f:\n",
        "            if line.startswith(\"#\\tpassage\"):\n",
        "                writer.write(line)\n",
        "            elif line == \"\" or line == \"\\n\":\n",
        "                writer.write(line)\n",
        "                if not predictions[example_id]:\n",
        "                    example_id += 1\n",
        "            elif predictions[example_id]:\n",
        "                cols = line.rstrip().split()\n",
        "                cols.append(predictions[example_id].pop(0))\n",
        "                writer.write(\"\\t\".join(cols) + \"\\n\")\n",
        "            else:\n",
        "                logger.warning(\n",
        "                    \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
        "                )\n",
        "\n",
        "def write_result(output, predictions, labels):\n",
        "  predictions = [j for i in predictions for j in i]\n",
        "  labels = [j for i in labels for j in i]\n",
        "  with open(output, \"w\") as writer:\n",
        "    for p, l in zip(predictions, labels):\n",
        "      writer.write(str(l) + \"\\t\" + str(p) + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "39T42L5QQL-s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "from transformers import set_seed\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def train(model_args, data_args, train_args):\n",
        "    # pdb.set_trace()\n",
        "    if (\n",
        "        os.path.exists(train_args.output_dir)\n",
        "        and os.listdir(train_args.output_dir)\n",
        "        and train_args.do_train\n",
        "        and not train_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({train_args.output_dir}) already exists and is not empty.\"\n",
        "             \" Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # logger = create_logger(name=\"train_prod\", save_dir=train_args.output_dir)\n",
        "    logger.info(\"Training/evaluation parameters %s\", train_args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(train_args.seed)\n",
        "\n",
        "    # Prepare prod-ext task\n",
        "    labels = get_labels(data_args.labels)\n",
        "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        id2label=label_map,\n",
        "        label2id={label: i for i, label in enumerate(labels)},\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast,\n",
        "    )\n",
        "    model_args.use_crf = False\n",
        "    if model_args.use_crf:\n",
        "        # model = BertCRFForTagging.from_pretrained(\n",
        "        #     model_args.model_name_or_path,\n",
        "        #     config=config,\n",
        "        #     cache_dir=model_args.cache_dir,\n",
        "        #     tagging_schema=\"BIO\",\n",
        "        #     use_cls=model_args.use_cls\n",
        "        # )\n",
        "        model = BertBiLSTMCRFForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            tagging_schema=\"BIO\",\n",
        "            use_cls=model_args.use_cls,\n",
        "            hidden_dim=config.hidden_size\n",
        "        )\n",
        "    else:\n",
        "        model = BertForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_cls=model_args.use_cls\n",
        "        )\n",
        "\n",
        "    # Get datasets\n",
        "    # pdb.set_trace()\n",
        "    train_dataset = (\n",
        "        ProdDataset(\n",
        "            # data_file=os.path.join(data_args.data_dir, \"train.txt\"),\n",
        "            data_file=data_args.data_dir_train,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache\n",
        "        )\n",
        "        if train_args.do_train\n",
        "        else None\n",
        "    )\n",
        "    eval_dataset = (\n",
        "        ProdDataset(\n",
        "            # data_file=os.path.join(data_args.data_dir, \"dev.txt\"),\n",
        "            data_file=data_args.data_dir_dev,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache\n",
        "        )\n",
        "        if train_args.do_eval\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    def compute_metrics(predictions, label_ids) -> Dict:\n",
        "        label_list = [[label_map[x] for x in seq] for seq in label_ids]\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "\n",
        "        return {\n",
        "            \"precision\": precision_score(label_list, preds_list),\n",
        "            \"recall\": recall_score(label_list, preds_list),\n",
        "            \"f1\": f1_score(label_list, preds_list),\n",
        "        }\n",
        "\n",
        "    metrics_fn = compute_metrics\n",
        "    dataset_len = len(train_dataset)\n",
        "    batch_size = train_args.per_device_train_batch_size\n",
        "    total_steps = (dataset_len // batch_size) * train_args.num_train_epochs if dataset_len % batch_size == 0 else \\\n",
        "        (dataset_len // batch_size + 1) * train_args.num_train_epochs\n",
        "    train_args.warmup_steps = 0.1 * total_steps\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = IETrainer(\n",
        "        model=model,\n",
        "        args=train_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=metrics_fn,\n",
        "        use_crf=model_args.use_crf,\n",
        "        optimizers=get_optimizer_grouped_parameters(\n",
        "            use_crf=model_args.use_crf,\n",
        "            args=train_args,\n",
        "            model=model,\n",
        "            num_training_steps=total_steps),\n",
        "            # num_training_steps=len(train_dataset) * train_args.num_train_epochs),\n",
        "        epoch=train_args.num_train_epochs\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if train_args.do_train:\n",
        "        trainer.train()\n",
        "        # Pass model_path to train() if continue training from an existing ckpt.\n",
        "        # trainer.train(\n",
        "        #     model_path=model_args.model_name_or_path\n",
        "        #     if os.path.isdir(model_args.model_name_or_path)\n",
        "        #     else None\n",
        "        # )\n",
        "        trainer.save_model()\n",
        "        tokenizer.save_pretrained(train_args.output_dir)\n",
        "\n",
        "    # Evaluation\n",
        "    if train_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        output = trainer.evaluate()\n",
        "        predictions = output['predictions']\n",
        "        label_ids = output['label_ids']\n",
        "        metrics = output['metrics']\n",
        "        output_eval_file = os.path.join(train_args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            for key, value in metrics.items():\n",
        "                logger.info(\"  %s = %s\", key, value)\n",
        "                writer.write(\"%s = %s\\n\" % (key, value))\n",
        "\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "        print(\"----------dev----------\")\n",
        "        print(metrics)\n",
        "        write_result(\"validation_result.txt\", output[\"predictions\"], output[\"label_ids\"])\n",
        "        # Save predictions\n",
        "        write_predictions(\n",
        "            data_args.data_dir_dev,\n",
        "            os.path.join(train_args.output_dir, \"eval_predictions.txt\"),\n",
        "            preds_list\n",
        "        )\n",
        "\n",
        "    # Predict\n",
        "    if train_args.do_predict:\n",
        "        test_dataset = ProdDataset(\n",
        "            data_file=data_args.data_dir_test,\n",
        "            tokenizer=tokenizer,\n",
        "            labels=labels,\n",
        "            model_type=config.model_type,\n",
        "            max_seq_length=data_args.max_seq_length,\n",
        "            overwrite_cache=data_args.overwrite_cache,\n",
        "        )\n",
        "\n",
        "        output = trainer.predict(test_dataset)\n",
        "\n",
        "        predictions = output['predictions']\n",
        "        label_ids = output['label_ids']\n",
        "        metrics = output['metrics']\n",
        "\n",
        "        preds_list = [[label_map[x] for x in seq] for seq in predictions]\n",
        "        print(\"----------test----------\")\n",
        "        print(metrics)\n",
        "        output_test_results_file = os.path.join(train_args.output_dir, \"test_results.txt\")\n",
        "        with open(output_test_results_file, \"w\") as writer:\n",
        "            for key, value in metrics.items():\n",
        "                logger.info(\"  %s = %s\", key, value)\n",
        "                writer.write(\"%s = %s\\n\" % (key, value))\n",
        "\n",
        "        # Save predictions\n",
        "        write_predictions(\n",
        "            data_args.data_dir_test,\n",
        "            os.path.join(train_args.output_dir, \"test_predictions.txt\"),\n",
        "            preds_list\n",
        "        )\n",
        "\n",
        "def get_optimizer_grouped_parameters(use_crf, args, model, num_training_steps):\n",
        "    pdb.set_trace()\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    if use_crf:\n",
        "        crf = \"crf\"\n",
        "        crf_lr = args.crf_learning_rate\n",
        "        logger.info(f\"Learning rate for CRF: {crf_lr}\")\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if (not any(nd in n for nd in no_decay)) and (crf not in n)\n",
        "                ],\n",
        "                \"weight_decay\": args.weight_decay\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for p in model.crf.parameters()],\n",
        "                \"weight_decay\": args.weight_decay,\n",
        "                \"lr\": crf_lr\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay) and (not crf not in n)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "    else:\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": args.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=args.learning_rate,\n",
        "        eps=args.adam_epsilon\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=args.warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def predict(model_args, predict_args):\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    # logger = create_logger(name=\"predict_prod\", save_dir=train_args.output_dir)\n",
        "    logger.info(\"Predict parameters %s\", predict_args)\n",
        "\n",
        "    # Prepare prod-ext task\n",
        "    labels = get_labels(predict_args.labels)\n",
        "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        id2label=label_map,\n",
        "        label2id={label: i for i, label in enumerate(labels)},\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast,\n",
        "    )\n",
        "    if model_args.use_crf:\n",
        "        # model = BertCRFForTagging.from_pretrained(\n",
        "        #     model_args.model_name_or_path,\n",
        "        #     config=config,\n",
        "        #     cache_dir=model_args.cache_dir,\n",
        "        #     tagging_schema=\"BIO\"\n",
        "        # )\n",
        "        model = BertBiLSTMCRFForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            tagging_schema=\"BIO\",\n",
        "            use_cls=model_args.use_cls,\n",
        "            hidden_dim=config.hidden_size\n",
        "        )\n",
        "    else:\n",
        "        model = BertForTagging.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir\n",
        "        )\n",
        "\n",
        "    device = torch.device(\n",
        "                \"cuda\"\n",
        "                if (not predict_args.no_cuda and torch.cuda.is_available())\n",
        "                else \"cpu\"\n",
        "            )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # load test dataset\n",
        "    test_dataset = ProdDataset(\n",
        "        data_file=predict_args.input_file,\n",
        "        tokenizer=tokenizer,\n",
        "        labels=labels,\n",
        "        model_type=config.model_type,\n",
        "        max_seq_length=predict_args.max_seq_length,\n",
        "        overwrite_cache=predict_args.overwrite_cache,\n",
        "    )\n",
        "\n",
        "    sampler = SequentialSampler(test_dataset)\n",
        "    data_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=sampler,\n",
        "        batch_size=predict_args.batch_size,\n",
        "        collate_fn=default_data_collator\n",
        "    )\n",
        "\n",
        "    logger.info(\"***** chuRunning Prediction *****\")\n",
        "    logger.info(\"  Num examples = {}\".format(len(data_loader.dataset)))\n",
        "    logger.info(\"  Batch size = {}\".format(predict_args.batch_size))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with open(predict_args.input_file, \"r\") as f:\n",
        "        all_preds = []\n",
        "        for inputs in tqdm(data_loader, desc=\"Predicting\"):\n",
        "            for k, v in inputs.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    inputs[k] = v.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    token_type_ids=inputs['token_type_ids']\n",
        "                )\n",
        "                logits = outputs[0]\n",
        "\n",
        "            preds = model.decode(logits, inputs['decoder_mask'].bool())\n",
        "            preds_list = [[label_map[x] for x in seq] for seq in preds]\n",
        "\n",
        "            all_preds += preds_list\n",
        "\n",
        "    write_predictions(\n",
        "        predict_args.input_file,\n",
        "        predict_args.output_file,\n",
        "        all_preds\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "nvoO38lMD8kV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import os\n",
        "\n",
        "from transformers import HfArgumentParser\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier.\"}\n",
        "    )\n",
        "    use_fast: bool = field(\n",
        "        default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "    use_crf: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether using CRF for inference.\"}\n",
        "    )\n",
        "    use_cls: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether concatenating token representation with [CLS].\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExTrainingArguments(TrainingArguments):\n",
        "    crf_learning_rate: float = field(\n",
        "        default=5e-3, metadata={\"help\": \"The initial learning rate of CRF parameters for Adam.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "    data_dir: str = field(\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_train: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_dev: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    data_dir_test: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
        "    )\n",
        "    labels: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    evaluate_during_training: bool = field(\n",
        "        default=False, metadata={\"help\": \"evaluate_during_training\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PredictArguments:\n",
        "    input_file: str = field(\n",
        "        metadata={\"help\": \"Path to a file containing sentences to be extracted (can be a single column file without labels).\"}\n",
        "    )\n",
        "    output_file: str = field(\n",
        "        default=\"/content\",\n",
        "        metadata={\"help\": \"Path to a file saving the outputs.\"}\n",
        "    )\n",
        "    labels: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Path to a file containing all labels.\"},\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                  \"than this will be truncated, sequences shorter will be padded.\"},\n",
        "    )\n",
        "    batch_size: int = field(\n",
        "        default=8, metadata={\"help\": \"Batch size for prediction.\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached test data.\"}\n",
        "    )\n",
        "    no_cuda: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Do not use CUDA even when it is available.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_train_args(args):\n",
        "    parser = HfArgumentParser((ModelArguments, DataArguments, ExTrainingArguments))\n",
        "    if len(args) == 1 and args[0].endswith(\".json\"):\n",
        "        model_args, data_args, train_args = parser.parse_json_file(\n",
        "            json_file=os.path.abspath(args[0]))\n",
        "    else:\n",
        "        model_args, data_args, train_args = parser.parse_args_into_dataclasses(args=args)\n",
        "\n",
        "    return model_args, data_args, train_args\n",
        "\n",
        "\n",
        "def parse_predict_args(args):\n",
        "    parser = HfArgumentParser((ModelArguments, PredictArguments))\n",
        "    if len(args) == 1 and args[0].endswith(\".json\"):\n",
        "        model_args, predict_args = parser.parse_json_file(\n",
        "            json_file=os.path.abspath(args[0]))\n",
        "    else:\n",
        "        model_args, predict_args = parser.parse_args_into_dataclasses(args=args)\n",
        "\n",
        "    return model_args, predict_args\n",
        "\n"
      ],
      "metadata": {
        "id": "9VdamH1NECu9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --no-cookies https://drive.google.com/uc?id=1--UFHJY3ldytNTsLcseIMDXEyzFeYEuj\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-oYW7JgHyhFj1HpbYvpHdsK8y-RwF2pW\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-KkDBrlyr3DF91BOTgp-vgCLwMe2JAmX\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=105JaXg7iAYF-0A7_srvSnaU0pfPE5ChH\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-6QHFiBYVWaNtbgCXdNbVfZ0WwdIB68N\n",
        "train_data = \"/content/train_optim_concat.txt\"\n",
        "dev_data = \"/content/dev_optim_concat.txt\"\n",
        "test_data = \"/content/test_optim_concat.txt\"\n",
        "prod_labels = \"/content/prod_labels.txt\"\n",
        "prod_train = \"/content/prod_train.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnnYGs6OJR3o",
        "outputId": "eeb7080b-49db-4af5-baae-b7aafdc922c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--UFHJY3ldytNTsLcseIMDXEyzFeYEuj\n",
            "To: /content/train_optim_concat.txt\n",
            "100% 1.57M/1.57M [00:00<00:00, 141MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-oYW7JgHyhFj1HpbYvpHdsK8y-RwF2pW\n",
            "To: /content/test_optim_concat.txt\n",
            "100% 167k/167k [00:00<00:00, 78.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-KkDBrlyr3DF91BOTgp-vgCLwMe2JAmX\n",
            "To: /content/dev_optim_concat.txt\n",
            "100% 162k/162k [00:00<00:00, 94.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=105JaXg7iAYF-0A7_srvSnaU0pfPE5ChH\n",
            "To: /content/prod_labels.txt\n",
            "100% 19.0/19.0 [00:00<00:00, 80.3kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-6QHFiBYVWaNtbgCXdNbVfZ0WwdIB68N\n",
            "To: /content/prod_train.json\n",
            "100% 504/504 [00:00<00:00, 2.50MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_args, data_args, train_args = parse_train_args([prod_train])\n",
        "\n",
        "model_args.model_name_or_path = \"jiangg/chembert_cased\"\n",
        "data_args.data_dir = \"/content\"\n",
        "data_args.data_dir_train = train_data\n",
        "data_args.data_dir_dev = dev_data\n",
        "data_args.data_dir_test = test_data\n",
        "data_args.labels = prod_labels\n",
        "train_args.per_device_train_batch_size = 32\n",
        "pdb.set_trace()\n",
        "train(model_args, data_args, train_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cg_et9eHEFf9",
        "outputId": "62ac49aa-55e2-4de3-9854-8b4cdb4e4fe7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 336, in set_trace\n",
            "    sys.settrace(self.trace_dispatch)\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Return--\n",
            "None\n",
            "> \u001b[0;32m<ipython-input-9-2e5781f4d0e5>\u001b[0m(10)\u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m      7 \u001b[0;31m\u001b[0mdata_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      8 \u001b[0;31m\u001b[0mdata_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprod_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m      9 \u001b[0;31m\u001b[0mtrain_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_device_train_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 10 \u001b[0;31m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     11 \u001b[0;31m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 347, in set_continue\n",
            "    sys.settrace(None)\n",
            "\n",
            "Some weights of BertForTagging were not initialized from the model checkpoint at jiangg/chembert_cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:__main__:Sequence length exceed 256 (cut).\n",
            "WARNING:__main__:Sequence length exceed 256 (cut).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[0;32m<ipython-input-6-b7ce8565d470>\u001b[0m(225)\u001b[0;36mget_optimizer_grouped_parameters\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    223 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mget_optimizer_grouped_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_crf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    224 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 225 \u001b[0;31m    \u001b[0mno_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LayerNorm.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    226 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0muse_crf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    227 \u001b[0;31m        \u001b[0mcrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"crf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> args\n",
            "use_crf = False\n",
            "args = ExTrainingArguments(output_dir='models/prod', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=19.3, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='models/prod/runs/Oct08_07-30-30_cfd3b77837db', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=100000, save_total_limit=None, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=12, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=1, dataloader_num_workers=0, past_index=-1, run_name='models/prod', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, crf_learning_rate=0.005)\n",
            "model = BertForTagging(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
            ")\n",
            "num_training_steps = 193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 361, in set_quit\n",
            "    sys.settrace(None)\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/debugger.py\", line 1075, in cmdloop\n",
            "    sys.settrace(None)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--KeyboardInterrupt--\n",
            "\n",
            "KeyboardInterrupt: Interrupted by user\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2e5781f4d0e5>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_device_train_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-b7ce8565d470>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_args, data_args, train_args)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Initialize our Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     trainer = IETrainer(\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-96639bd17311>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, compute_metrics, optimizers, use_crf, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     ):\n\u001b[0;32m---> 41\u001b[0;31m         super(IETrainer, self).__init__(\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBITS_AND_BYTES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         ):\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2177\u001b[0m             )\n\u001b[1;32m   2178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}